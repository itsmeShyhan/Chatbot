# -*- coding: utf-8 -*-
"""Copy of Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H248BG6fWGeQV9_c8jjCfCQsW3poYCsV
"""

pip install nltk

"""# Prototype"""

import nltk

nltk.download('all')

from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.stem.porter import PorterStemmer
import numpy as np
import json
import torch
import torch.nn  as nn
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')

# %cd /content/drive/MyDrive/Colab Notebooks

"""# NLP Techniques"""

def tokenizer(text):
  # print(word_tokenize(text))
  return word_tokenize(text)


def stemming(text):
  porter = PorterStemmer()
  stemmedWords = []
  # print(porter.stem(text[0]))
  # for i in text:
  #   # print(porter.stem(text[i]))
  #   stemmedWords.append(porter.stem(i))
  # print(stemmedWords)
  return porter.stem(text)



def bag_of_words(tokenized_sentence, all_words):
  # Stemming the already tokenized words
  stemmed_sentence = [stemming(w) for w in tokenized_sentence]


  # Creating an array of bag words based on the length of all words in intents
  bag = np.zeros(len(all_words), dtype=np.float32)
  for idx, w in enumerate(all_words):
    if w in stemmed_sentence:
      bag[idx] = 1.0
  return bag

"""# Data Preprocessing"""

# Loading the data from intents
with open('intents.json', 'r') as f:
  intents = json.load(f)

def dataPreprocessing():
  # Data required for training
  tags = []
  all_words = []
  xy = []


  for intent in intents['intents']:
    # Storing the tags
    tag = intent['tag']
    tags.append(tag)

    for pattern in intent["patterns"]:
      # tokenizing the words from intents
      tokenized_word = tokenizer(pattern)

      all_words.extend(tokenized_word)
      xy.append((tokenized_word, tag))

  # stem and lower each word
  ignore_words = ['?', '.', '!']
  all_words = [stemming(w) for w in all_words if w not in ignore_words]

  # remove duplicates and sort
  all_words = sorted(set(all_words))
  tags = sorted(set(tags))

  # getting the x_train (e.g. [0, 1, 0, 1, 1]) and y_train (e.g. "greeting" which will be in index number like 1)
  x_train = []
  y_train = []

  print ("inside bag of words call")
  for (sentence, tag) in xy:
    bag = bag_of_words(sentence, all_words)
    x_train.append(bag)
    y_train.append(tags.index(tag))


  x_train = np.array(x_train)
  y_train = np.array(y_train)

  class ChatDataset(Dataset):
    def __init__(self):
      self.n_samples = len(x_train)
      self.x_data = x_train
      self.y_data = y_train

    def __getitem__(self, index):
      return self.x_data[index], self.y_data[index]

    def __len__(self):
      return self.n_samples

  # preparing a dataset basd on the json data
  dataset = ChatDataset()

  return dataset, x_train, y_train, tags, all_words

"""# Model and Training"""

# Creating the model class
class neuralNetwork(nn.Module):
  def __init__(self, input_size, output_size, hidden_size):

    # initializing the constructor of the parent class
    super(neuralNetwork, self).__init__()

    # creating layers of the model as attibutes
    self.layer1 = nn.Linear(input_size, hidden_size)
    self.layer2 = nn.Linear(hidden_size, hidden_size)
    self.layer3 = nn.Linear(hidden_size, output_size)
    self.relu = nn.ReLU()

    # forward propogation
  def forward(self, x):
    out = self.layer1(x)
    out = self.relu(out)
    out = self.layer2(out)
    out = self.relu(out)
    out = self.layer3(out)

    return out


# Implementing the data preprocessing
(dataset, x_train, y_train, tags, all_words) = dataPreprocessing()


# trainloader helps iterate the dataset into batches
train_Loader = DataLoader(dataset=dataset, batch_size=8, shuffle= True, num_workers=2)

# creating the model
model= neuralNetwork(input_size=len(x_train[0]), output_size=len(tags), hidden_size=8)

training_loss = []

  # training the model
def train_model(model, train_Loader, epochs):
  # Loss function and optimizer
  criterion = nn.CrossEntropyLoss()
  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

  # training the model
  for epoch in range(epochs):
    for (words, label) in train_Loader:
      # forward propogation
      output = model(words)
      loss = criterion(output, label)

      # Backward and optimizing
      # Zeroing the gradient before optimizing
      optimizer.zero_grad()


      # calcute the loss/gradient
      loss.backward()

      # optimize the parameters based on gradient
      optimizer.step()


# printing the progress of epochs
    if (epoch +1) % 100 == 0:
      print(f"epoch {epoch+1}/{epochs}, loss={loss.item():.4f}")
      training_loss.append(loss.item())

  print(f"final loss, loss={loss.item():.4f}")
  plt.plot(training_loss, label = "training loss")
  plt.legend()
  plt.title("loss over epochs")
  plt.show()

train_model(model, train_Loader, epochs=1500)

"""# Saving and Loading Model"""

model.eval()

# saving the file
data = {
    "model_state": model.state_dict(),
    "input_size": len(x_train[0]),
    "output_size": len(tags),
    "hidden_size": 8,
    "all_words": all_words,
    "tags": tags
}

file = "saved.pth"
torch.save(data, file)

print(f"training complete. file saved to {file}")

# loading file

file2 = "saved.pth"
data = torch.load(file2)

input_size = data["input_size"]
output_size = data["output_size"]
hidden_size = data["hidden_size"]
all_words = data["all_words"]
tags = data["tags"]
model_state = data["model_state"]

# creating model based on the configurations
newModel = neuralNetwork(input_size, output_size, hidden_size)
newModel.load_state_dict(model_state)
newModel.eval()

"""# Using the model and running the chatbot"""

import random

# The chatbot part
bot_name = "MassyBot"

print("Welcome to MassyBot from Al Mass Indian Restaurant!")

while True:
  # getting user input
  user_input = input('User: ')

  # tokenize
  tokenized_input = tokenizer(user_input)

  # create bag of words
  bagged = bag_of_words(tokenized_input, all_words)
  bagged = bagged.reshape(1, bagged.shape[0])
  bagged = torch.from_numpy(bagged)

  # run model and get prediction tag
  output = newModel(bagged)
  _, prediction = torch.max(output, dim=1)
  tag = tags[prediction.item()]

  # getting the probability by adding a softmax layer (it is not there in eval but only in training)
  probs = torch.softmax(output, dim=1)
  prob = probs[0][prediction.item()]
  print(prob)

  # print(prediction
  # based on prediction print response
  if prob > 0.5:
    for intent in intents['intents']:
      if intent['tag'] == tag:
        print(f"{bot_name}: {random.choice(intent['responses'])}")
  else:
    print("I do not understand...")

"""## Implementation Reference

1. Loeber,  P. (2020). PyTorch Bot Creation Playlist. Available at: https://youtube.com/playlist?list=PLC-Eil48AiqUlPLB8HGRABtydUOFL84DH&si=-axfL_ZbfFNFPzht [Accessed on 7 January 2025]

2. Geeksforgeeks (2023). Create model using
 custom moduel in pytorch. Available from: https://www.geeksforgeeks.org/create-model-using-custom-module-in-pytorch/ [Accessed on 14 January 2025]

3. Mulla, R. (2023). Build Your First Pytorch Model In Minutes! [Tutorial + Code]. Available at: https://www.youtube.com/watch?v=tHL5STNJKag&t=1615s [Accessed on 14 January 2025]
"""